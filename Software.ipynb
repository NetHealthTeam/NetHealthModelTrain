{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Software.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKdGZNKWkpfs"
      },
      "source": [
        "## First Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvntGlg26ptq",
        "outputId": "abfa47b0-58c0-4dcf-8175-a22f59b07c0b"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh2WRGVacpBG"
      },
      "source": [
        "our_dict = {0:\"workout\",1:\"diet for next days\",2:\"diet for today\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmI9Sj-g6wgu"
      },
      "source": [
        "data = pd.read_csv('data.csv',header=None,delimiter=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "8OFKwpBQ0UR9",
        "outputId": "0c744686-c621-4969-898c-e871d37b5f52"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>how should I work out for jul 1th</td>\n",
              "      <td>0</td>\n",
              "      <td>jul 1th</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is the workout on aug 3rd</td>\n",
              "      <td>0</td>\n",
              "      <td>aug 3rd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what should I work out today</td>\n",
              "      <td>0</td>\n",
              "      <td>today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hey what should I work out monday</td>\n",
              "      <td>0</td>\n",
              "      <td>monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hello how should I work out for june 2nd</td>\n",
              "      <td>0</td>\n",
              "      <td>june 2nd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          0  1         2\n",
              "0         how should I work out for jul 1th  0   jul 1th\n",
              "1            what is the workout on aug 3rd  0   aug 3rd\n",
              "2              what should I work out today  0     today\n",
              "3         Hey what should I work out monday  0    monday\n",
              "4  hello how should I work out for june 2nd  0  june 2nd"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "6mpaXA0d0X0h",
        "outputId": "5c769bf7-4a70-47ae-f5a2-756aa13cb693"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>hello what is the meal for lunch</td>\n",
              "      <td>2</td>\n",
              "      <td>lunch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>what's the diet for breakfast</td>\n",
              "      <td>2</td>\n",
              "      <td>breakfast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>hi what do I have to eat for breakfast</td>\n",
              "      <td>2</td>\n",
              "      <td>breakfast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>what's the diet plan for lunch</td>\n",
              "      <td>2</td>\n",
              "      <td>lunch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>hey what's the meal for breakfast</td>\n",
              "      <td>2</td>\n",
              "      <td>breakfast</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          0  1          2\n",
              "395        hello what is the meal for lunch  2      lunch\n",
              "396           what's the diet for breakfast  2  breakfast\n",
              "397  hi what do I have to eat for breakfast  2  breakfast\n",
              "398          what's the diet plan for lunch  2      lunch\n",
              "399       hey what's the meal for breakfast  2  breakfast"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTzfrSd16rbG"
      },
      "source": [
        "X, y = data[0],data[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuEbuT2V7mOr",
        "outputId": "3aff6cce-8e1f-4410-83db-eadc10d3de6e"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             how should I work out for jul 1th\n",
              "1                what is the workout on aug 3rd\n",
              "2                  what should I work out today\n",
              "3             Hey what should I work out monday\n",
              "4      hello how should I work out for june 2nd\n",
              "                         ...                   \n",
              "395            hello what is the meal for lunch\n",
              "396               what's the diet for breakfast\n",
              "397      hi what do I have to eat for breakfast\n",
              "398              what's the diet plan for lunch\n",
              "399           hey what's the meal for breakfast\n",
              "Name: 0, Length: 400, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7txjDBUDDs_",
        "outputId": "7db7aab5-b00e-4b7f-96c2-db8109058083"
      },
      "source": [
        "months = \"january|february|march|april|may|june|july|august|september|octember|november|december\"\n",
        "months = months.split(\"|\")\n",
        "print(months)\n",
        "vocab = \"\"\n",
        "for month in months:\n",
        "  for i in range(len(month)+1):\n",
        "    vocab = vocab + month[:i] + \"|\"\n",
        "  vocab = vocab[:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'octember', 'november', 'december']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wByUg4KRqmLj"
      },
      "source": [
        "months = {\"january\":1,\"february\":2,\"march\":3,\"april\":4,\"may\":5,\"june\":6,\"july\":7,\"august\":8,\"september\":9,\"octember\":10,\"november\":11,\"december\":12}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbKJrVZDFibn"
      },
      "source": [
        "def make_vocab():\n",
        "  months = \"january|february|march|april|may|june|july|august|september|octember|november|december\"\n",
        "  months = months.split(\"|\")\n",
        "  vocab = \"\"\n",
        "  time = 0\n",
        "  while True:\n",
        "    for month in months:\n",
        "      if len(month)>=3:\n",
        "        vocab = vocab + month + \"|\"\n",
        "    for i in range(len(months)):\n",
        "      if months[i] != \"\":\n",
        "        months[i] = months[i][:-1]\n",
        "      else: time +=1\n",
        "    \n",
        "    if time > 11:\n",
        "      break\n",
        "    else: time = 0\n",
        "  return vocab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ou1G3wAc5O"
      },
      "source": [
        "regex = \"\\w*\\d+\\w{0,3}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7aHYKHKAMUI"
      },
      "source": [
        "m = re.match(r\".*(\\d+).*(\"+vocab[:-1]+\")\", \" the 3rd of jul\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEsKEXL2AT8z",
        "outputId": "f8a729fe-77c9-475d-dfb4-42e3e90311ea"
      },
      "source": [
        "print(m.group(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GUMTjVy3ppD"
      },
      "source": [
        "# date = [\"january\",\"february\",\"march,\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNgXx_dWuCaO"
      },
      "source": [
        "def find_month(x,vocab):\n",
        "  m = re.match(r\".*(\\d+).*(\"+vocab[:-1]+\")\", x)\n",
        "  if m:\n",
        "    return m.group(1),m.group(2)\n",
        "  else:\n",
        "    m = re.match(r\".*(\"+vocab[:-1]+\").*(\\d+)\", x)\n",
        "    if m: return m.group(2),m.group(1)\n",
        "    else: return None,None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jCIUitIMhOi"
      },
      "source": [
        "def find_day(x):\n",
        "  m = re.match(r\".*(today|tomorrow|yesterday|next day|nextday)\", x)\n",
        "  if m:\n",
        "    return m.group(1)\n",
        "  else: return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJyh16rWNQdz"
      },
      "source": [
        "def find_week_day(x):\n",
        "  m = re.match(r\".*(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\", x)\n",
        "  if m:\n",
        "    return m.group(1)\n",
        "  else: return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc9weVRkTOpd"
      },
      "source": [
        "def find_meal_time(x):\n",
        "  m = re.match(r\".*(lunch|dinner|supper|brunch|breakfast)\", x)\n",
        "  if m:\n",
        "    return m.group(1)\n",
        "  else: return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXGc0-s845Rg",
        "outputId": "c4d98a27-b26e-4df9-c5ba-f6510d2f5f1c"
      },
      "source": [
        "from datetime import datetime \n",
        "\n",
        "# to get hour from datetime\n",
        "print('Hour: ', todays_date.year)\n",
        "\n",
        "# to get minute from datetime\n",
        "print('Minute: ', todays_date.month)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hour:  2021\n",
            "Minute:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REWAnuME7YDi"
      },
      "source": [
        "def make_key_month(months):\n",
        "  # print(months.items)\n",
        "  new_dict = {}\n",
        "  for key,value in months.items():\n",
        "    word = key\n",
        "    i = 0\n",
        "\n",
        "    # print(word)\n",
        "    while len(word) >= 3:\n",
        "      \n",
        "      new_dict[word] = value\n",
        "      i += 1\n",
        "      word = key[:-i]\n",
        "\n",
        "      # print(new_dict)\n",
        "  print(new_dict)\n",
        "  return new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuRRS3uc5Jzl",
        "outputId": "043be7b8-e24c-41e9-8e3a-ca678ad070ec"
      },
      "source": [
        "my_string = '2019-10-31'\n",
        "\n",
        "# Create date object in given time format yyyy-mm-dd\n",
        "my_date = datetime.strptime(my_string, \"%Y-%m-%d\")\n",
        "\n",
        "print(my_date)\n",
        "print('Type: ',type(my_date))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2019-10-31 00:00:00\n",
            "Type:  <class 'datetime.datetime'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYOcinwjt5OW"
      },
      "source": [
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import calendar\n",
        "\n",
        "\n",
        "def find_date(X):\n",
        "  months = {\"january\":1,\"february\":2,\"march\":3,\"april\":4,\"may\":5,\"june\":6,\"july\":7,\"august\":8,\"september\":9,\"octember\":10,\"november\":11,\"december\":12}\n",
        "  days_not_digit = {\"today\":0,\"tomorrow\":1,\"next day\":1,\"nextday\":1}\n",
        "  week_days = {\"monday\":1,\"tuesday\":2,\"wednesday\":3,\"thursday\":4,\"friday\":5,\"saturday\":6,\"sunday\":7}\n",
        "  '''monday|tuesday|wednesday|thursday|friday|saturday|sunday'''\n",
        "  week_days\n",
        "  months = make_key_month(months)\n",
        "  todays_date = datetime.now()\n",
        "  day_type = {}\n",
        "  vocab = make_vocab()\n",
        "  print(f\"Vocab is\\n{vocab}\")\n",
        "  print(f\"sentence:\\n{X}\")\n",
        "  day, month = find_month(X,vocab)\n",
        "  # if not day:\n",
        "  day_not_digit = find_day(X)\n",
        "  # if not day:\n",
        "  week_day = find_week_day(X)\n",
        "  # if not day:\n",
        "  meal_time = find_meal_time(X)\n",
        "    \n",
        "    # print(\"Date is\")\n",
        "  if day and month:\n",
        "    my_string = str(todays_date.year)+\"-\"+str(months[month])+\"-\"+str(day)\n",
        "    my_date = datetime.strptime(my_string, \"%Y-%m-%d\")  \n",
        "    # print(my_date)\n",
        "  if day_not_digit:\n",
        "    digit = days_not_digit[day_not_digit]\n",
        "    date_diet = datetime.today() + timedelta(days=digit)\n",
        "    my_string = str(date_diet.year)+\"-\"+str(date_diet.month)+\"-\"+str(date_diet.day)\n",
        "    my_date = datetime.strptime(my_string, \"%Y-%m-%d\")  \n",
        "    # print(my_date)\n",
        "  if week_day:\n",
        "    distance = week_days[week_day] - week_days[str(calendar.day_name[todays_date.weekday()]).lower()]\n",
        "    if distance < 0:\n",
        "      distance += 7\n",
        "    date_diet = datetime.today() + timedelta(days=distance)\n",
        "    my_string = str(date_diet.year)+\"-\"+str(date_diet.month)+\"-\"+str(date_diet.day)\n",
        "    my_date = datetime.strptime(my_string, \"%Y-%m-%d\")  \n",
        "\n",
        "    return my_date,meal_time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23GuDDP8uHq7",
        "outputId": "e1cb24d9-1036-4e0f-d201-834f59e25f11"
      },
      "source": [
        "\n",
        "print(find_date(X[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'january': 1, 'januar': 1, 'janua': 1, 'janu': 1, 'jan': 1, 'february': 2, 'februar': 2, 'februa': 2, 'febru': 2, 'febr': 2, 'feb': 2, 'march': 3, 'marc': 3, 'mar': 3, 'april': 4, 'apri': 4, 'apr': 4, 'may': 5, 'june': 6, 'jun': 6, 'july': 7, 'jul': 7, 'august': 8, 'augus': 8, 'augu': 8, 'aug': 8, 'september': 9, 'septembe': 9, 'septemb': 9, 'septem': 9, 'septe': 9, 'sept': 9, 'sep': 9, 'octember': 10, 'octembe': 10, 'octemb': 10, 'octem': 10, 'octe': 10, 'oct': 10, 'november': 11, 'novembe': 11, 'novemb': 11, 'novem': 11, 'nove': 11, 'nov': 11, 'december': 12, 'decembe': 12, 'decemb': 12, 'decem': 12, 'dece': 12, 'dec': 12}\n",
            "Vocab is\n",
            "january|february|march|april|may|june|july|august|september|octember|november|december|januar|februar|marc|apri|jun|jul|augus|septembe|octembe|novembe|decembe|janua|februa|mar|apr|augu|septemb|octemb|novemb|decemb|janu|febru|aug|septem|octem|novem|decem|jan|febr|septe|octe|nove|dece|feb|sept|oct|nov|dec|sep|\n",
            "sentence:\n",
            "how should I work out for jul 1th\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgFZv8hKsyDQ"
      },
      "source": [
        "def convert_date(X):\n",
        "  find"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfk9N2IA7p5N",
        "outputId": "c7a51b6a-618f-4f57-a793-086733943535"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "395    2\n",
              "396    2\n",
              "397    2\n",
              "398    2\n",
              "399    2\n",
              "Name: 1, Length: 400, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6eDTNwX7q61",
        "outputId": "2715b531-75a0-42ad-8d8e-990e47a8a938"
      },
      "source": [
        "documents = []\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TceI3Vl47w48",
        "outputId": "ca76cf91-5cd3-4996-acec-8c975d23ce88"
      },
      "source": [
        "documents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how should work out for jul 1th',\n",
              " 'what is the workout on aug 3rd',\n",
              " 'what should work out today',\n",
              " 'hey what should work out monday',\n",
              " 'hello how should work out for june 2nd',\n",
              " 'hey what the work out april 3rd',\n",
              " 'what is the work out tomorrow',\n",
              " 'how should work out for feb 2nd',\n",
              " 'what should work out jun 16',\n",
              " 'hello how should work out for monday',\n",
              " 'hey what should work out february 2nd',\n",
              " 'how should work out for friday',\n",
              " 'how should work out for dec 29th',\n",
              " 'hello how should work out for feb 3rd',\n",
              " 'what should work out the next day',\n",
              " 'hello what should work out january 2nd',\n",
              " 'what is the workout plan on monday',\n",
              " 'what is the workout plan on the next day',\n",
              " 'what should work out nov 24th',\n",
              " 'hi how should work out for saturday',\n",
              " 'how should work out for sunday',\n",
              " 'hello how should work out for the next day',\n",
              " 'what the work out plan on aug 16th',\n",
              " 'what should work out mar 2nd',\n",
              " 'hello what should work out tomorrow',\n",
              " 'hey how should work out for may 24th',\n",
              " 'hello what is the workout plan may 1th',\n",
              " 'hey what should work out sunday',\n",
              " 'hi what should work out saturday',\n",
              " 'how should work out for october 3rd',\n",
              " 'what should work out thursday',\n",
              " 'hey what the workout tomorrow',\n",
              " 'how should work out for today',\n",
              " 'hello what should work out september 3rd',\n",
              " 'hi what the workout sunday',\n",
              " 'what should work out march 2nd',\n",
              " 'hey how should work out for tuesday',\n",
              " 'hey what should work out saturday',\n",
              " 'how should work out for the next day',\n",
              " 'hey how should work out for aug 1th',\n",
              " 'what the work out on tomorrow',\n",
              " 'hey what should work out tuesday',\n",
              " 'hey what is the workout the next day',\n",
              " 'what should work out monday',\n",
              " 'what the work out the next day',\n",
              " 'what the work out friday',\n",
              " 'what the workout plan on oct 09th',\n",
              " 'what is the workout today',\n",
              " 'what the work out plan wednesday',\n",
              " 'what is the workout march 1th',\n",
              " 'what the workout april 3rd',\n",
              " 'hello what the workout january 17',\n",
              " 'what is the workout the next day',\n",
              " 'what should work out dec 2nd',\n",
              " 'hi how should work out for december 18',\n",
              " 'hello what should work out monday',\n",
              " 'hi what should work out the next day',\n",
              " 'what is the work out on monday',\n",
              " 'what is the workout plan on tomorrow',\n",
              " 'hi how should work out for sunday',\n",
              " 'how should work out for december 1th',\n",
              " 'hello what is the workout tomorrow',\n",
              " 'what should work out jul 3rd',\n",
              " 'what is the workout on today',\n",
              " 'how should work out for saturday',\n",
              " 'hey what the work out on today',\n",
              " 'how should work out for tomorrow',\n",
              " 'hey what should work out thursday',\n",
              " 'hi what should work out today',\n",
              " 'hi what should work out friday',\n",
              " 'hi what the workout friday',\n",
              " 'hello how should work out for tomorrow',\n",
              " 'how should work out for wednesday',\n",
              " 'hey how should work out for sunday',\n",
              " 'how should work out for nov 1th',\n",
              " 'hey what should work out today',\n",
              " 'what the work out plan on wednesday',\n",
              " 'hey what should work out october 3rd',\n",
              " 'hello what should work out jun 1th',\n",
              " 'how should work out for september 17',\n",
              " 'what should work out aug 2nd',\n",
              " 'hello what should work out the next day',\n",
              " 'hey how should work out for tomorrow',\n",
              " 'what is the work out monday',\n",
              " 'hello how should work out for friday',\n",
              " 'hi how should work out for tomorrow',\n",
              " 'what the work out on sunday',\n",
              " 'hello how should work out for jun 06',\n",
              " 'what the workout plan december 2nd',\n",
              " 'what the work out on july 2nd',\n",
              " 'what is the work out on today',\n",
              " 'hi what should work out jul 27th',\n",
              " 'hey what is the work out plan november 3rd',\n",
              " 'hi what should work out sunday',\n",
              " 'how should work out for monday',\n",
              " 'what is the work out plan on thursday',\n",
              " 'hi what should work out jan 2nd',\n",
              " 'hey how should work out for today',\n",
              " 'hello how should work out for thursday',\n",
              " 'hello what is the workout plan thursday',\n",
              " 'what should work out sep 2nd',\n",
              " 'what should work out apr 3rd',\n",
              " 'hey what the work out plan january 29th',\n",
              " 'hi how should work out for the next day',\n",
              " 'hello how should work out for jan 3rd',\n",
              " 'hi what should work out february 1th',\n",
              " 'hello what should work out january 1th',\n",
              " 'how should work out for may 2nd',\n",
              " 'what the workout plan today',\n",
              " 'hi what should work out thursday',\n",
              " 'what is the work out plan on august 2nd',\n",
              " 'how should work out for apr 07th',\n",
              " 'hello how should work out for nov 14th',\n",
              " 'hello what should work out may 3rd',\n",
              " 'hey what should work out wednesday',\n",
              " 'hey what should work out august 09',\n",
              " 'hi what the workout plan on today',\n",
              " 'what the workout plan february 3rd',\n",
              " 'hello what should work out today',\n",
              " 'hey what is the workout plan the next day',\n",
              " 'hello what the work out plan on tomorrow',\n",
              " 'how should work out for thursday',\n",
              " 'hello what is the workout plan the next day',\n",
              " 'how should work out for feb 3rd',\n",
              " 'hi what the work out plan today',\n",
              " 'hello what the workout plan monday',\n",
              " 'hi how should work out for may 2nd',\n",
              " 'what the workout the next day',\n",
              " 'hey what is the work out jan 1th',\n",
              " 'how should work out for march 3rd',\n",
              " 'what is the workout plan on tuesday',\n",
              " 'what should work out tomorrow',\n",
              " 'what the workout on tuesday',\n",
              " 'hi what should work out monday',\n",
              " 'what the work out sep 04th',\n",
              " 'what should work out aug 3rd',\n",
              " 'hi how should work out for today',\n",
              " 'hey how should work out for december 24th',\n",
              " 'what should work out sunday',\n",
              " 'hi what the work out plan wednesday',\n",
              " 'what is the work out plan on january 1th',\n",
              " 'hello what is the workout on the next day',\n",
              " 'hey what the work out on thursday',\n",
              " 'how should work out for september 24th',\n",
              " 'what the workout apr 1th',\n",
              " 'how should work out for february 2nd',\n",
              " 'what should work out tuesday',\n",
              " 'what should work out feb 09th',\n",
              " 'what should work out apr 1th',\n",
              " 'hey what the workout plan on saturday',\n",
              " 'hey what should work out march 18th',\n",
              " 'hi how should work out for monday',\n",
              " 'hello what the workout plan sunday',\n",
              " 'hello what is the work out plan tomorrow',\n",
              " 'hi what should work out february 3rd',\n",
              " 'what do have to eat wednesday',\n",
              " 'hello what is the meal for the next day',\n",
              " 'what the meal for may 19',\n",
              " 'what can eat tomorrow',\n",
              " 'hey what do have to eat feb 3rd',\n",
              " 'hey what should eat the next day',\n",
              " 'what should eat march 2nd',\n",
              " 'hi what do have to eat the next day',\n",
              " 'what the meal for february 1th',\n",
              " 'hello what can eat saturday',\n",
              " 'what is the meal for tomorrow',\n",
              " 'hey what do have to eat apr 2nd',\n",
              " 'hi what is the meal for the next day',\n",
              " 'what do have to eat sunday',\n",
              " 'what should eat today',\n",
              " 'hi what do have to eat tomorrow',\n",
              " 'hello what the meal for saturday',\n",
              " 'what do have to eat the next day',\n",
              " 'what can eat aug 1th',\n",
              " 'what should eat saturday',\n",
              " 'hello what should eat thursday',\n",
              " 'hello what should eat wednesday',\n",
              " 'what do have to eat tuesday',\n",
              " 'hi what do have to eat apr 1th',\n",
              " 'what the diet for friday',\n",
              " 'what can eat tuesday',\n",
              " 'what can eat the next day',\n",
              " 'what is the diet for oct 26',\n",
              " 'hey what do have to eat aug 2nd',\n",
              " 'what do have to eat december 3rd',\n",
              " 'hi what do have to eat tuesday',\n",
              " 'hey what can eat sunday',\n",
              " 'what should eat tomorrow',\n",
              " 'what is the meal for wednesday',\n",
              " 'hi what do have to eat aug 18',\n",
              " 'hello what should eat today',\n",
              " 'hi what can eat today',\n",
              " 'hi what can eat sep 1th',\n",
              " 'hey what the meal for april 07th',\n",
              " 'hey what the diet plan for saturday',\n",
              " 'what can eat aug 3rd',\n",
              " 'hey what can eat today',\n",
              " 'hi what do have to eat december 1th',\n",
              " 'hello what can eat jun 1th',\n",
              " 'hello what can eat sep 09',\n",
              " 'hello what should eat november 2nd',\n",
              " 'hi what can eat september 27th',\n",
              " 'hi what the diet for tomorrow',\n",
              " 'what can eat february 2nd',\n",
              " 'what should eat jan 1th',\n",
              " 'hey what should eat wednesday',\n",
              " 'what should eat the next day',\n",
              " 'hi what the meal for the next day',\n",
              " 'hi what should eat monday',\n",
              " 'hello what can eat the next day',\n",
              " 'what the diet for thursday',\n",
              " 'hello what the diet plan for saturday',\n",
              " 'what is the diet plan for thursday',\n",
              " 'what is the meal for april 3rd',\n",
              " 'hey what should eat dec 1th',\n",
              " 'hi what should eat friday',\n",
              " 'what do have to eat tomorrow',\n",
              " 'what can eat today',\n",
              " 'hey what is the diet plan for thursday',\n",
              " 'what do have to eat thursday',\n",
              " 'hello what should eat november 3rd',\n",
              " 'what is the diet for the next day',\n",
              " 'hi what can eat tuesday',\n",
              " 'hello what should eat july 1th',\n",
              " 'hey what should eat dec 06th',\n",
              " 'hi what is the diet plan for jun 2nd',\n",
              " 'hi what can eat tomorrow',\n",
              " 'what can eat thursday',\n",
              " 'what do have to eat apr 04',\n",
              " 'what should eat friday',\n",
              " 'hi what should eat april 04th',\n",
              " 'hello what do have to eat jan 2nd',\n",
              " 'what do have to eat jan 3rd',\n",
              " 'hi what do have to eat friday',\n",
              " 'what can eat october 2nd',\n",
              " 'what can eat friday',\n",
              " 'what should eat thursday',\n",
              " 'hello what do have to eat friday',\n",
              " 'what can eat dec 3rd',\n",
              " 'what can eat saturday',\n",
              " 'hello what should eat september 1th',\n",
              " 'what can eat wednesday',\n",
              " 'hey what can eat may 05',\n",
              " 'what can eat september 3rd',\n",
              " 'hey what should eat jun 1th',\n",
              " 'hi what should eat mar 07th',\n",
              " 'what the meal for thursday',\n",
              " 'hey what the diet plan for jun 3rd',\n",
              " 'hey what do have to eat tomorrow',\n",
              " 'what can eat november 1th',\n",
              " 'what do have to eat monday',\n",
              " 'hi what can eat october 3rd',\n",
              " 'hey what do have to eat oct 2nd',\n",
              " 'hello what can eat february 3rd',\n",
              " 'what the diet plan for the next day',\n",
              " 'what the diet for dec 1th',\n",
              " 'what do have to eat today',\n",
              " 'hi what should eat mar 1th',\n",
              " 'hello what do have to eat the next day',\n",
              " 'what the diet plan for monday',\n",
              " 'what do have to eat april 1th',\n",
              " 'what can eat april 15',\n",
              " 'what should eat sunday',\n",
              " 'hello what should eat feb 2nd',\n",
              " 'hello what the meal for friday',\n",
              " 'hey what do have to eat today',\n",
              " 'what can eat jan 3rd',\n",
              " 'hey what is the diet plan for wednesday',\n",
              " 'what the meal for october 2nd',\n",
              " 'hey what do have to eat oct 1th',\n",
              " 'hi what should eat today',\n",
              " 'what is the diet for tomorrow',\n",
              " 'hello what can eat tomorrow',\n",
              " 'hello what the diet plan for dec 2nd',\n",
              " 'what the diet for tomorrow',\n",
              " 'hi what do have to eat mar 1th',\n",
              " 'what the diet for today',\n",
              " 'hi what do have to eat january 2nd',\n",
              " 'hello what the diet for tomorrow',\n",
              " 'hey what do have to eat october 2nd',\n",
              " 'hi what is the diet plan for monday',\n",
              " 'what is the diet for february 2nd',\n",
              " 'what can eat december 1th',\n",
              " 'what the meal for tomorrow',\n",
              " 'what can eat march 1th',\n",
              " 'hello what is the meal for nov 1th',\n",
              " 'what should eat tuesday',\n",
              " 'hey what should eat tomorrow',\n",
              " 'hey what is the diet for saturday',\n",
              " 'hello what do have to eat aug 2nd',\n",
              " 'hi what should eat mar 29',\n",
              " 'hi what should eat august 28th',\n",
              " 'hello what should eat mar 2nd',\n",
              " 'what do have to eat friday',\n",
              " 'what is the diet for today',\n",
              " 'hi what should eat sunday',\n",
              " 'hey what do have to eat sunday',\n",
              " 'hey what can eat march 24th',\n",
              " 'what the meal for september 1th',\n",
              " 'what should eat february 3rd',\n",
              " 'hey what is the diet for tomorrow',\n",
              " 'hey what can eat tomorrow',\n",
              " 'hi what should eat jul 3rd',\n",
              " 'hello what do have to eat tomorrow',\n",
              " 'hey what should eat jul 1th',\n",
              " 'what can eat sep 2nd',\n",
              " 'hi what the diet plan for the next day',\n",
              " 'hey what do have to eat february 3rd',\n",
              " 'what the meal for sep 15',\n",
              " 'what do have to eat saturday',\n",
              " 'what the diet plan for dec 3rd',\n",
              " 'what the meal for april 1th',\n",
              " 'hi what the diet plan for dec 2nd',\n",
              " 'what the meal for today',\n",
              " 'hey what do have to eat for dinner',\n",
              " 'hello what do have to eat for dinner',\n",
              " 'hi what is the meal for dinner',\n",
              " 'what do have to eat for supper',\n",
              " 'what is the diet for brunch',\n",
              " 'what do have to eat for breakfast',\n",
              " 'hey what the meal for lunch',\n",
              " 'what do have to eat for brunch',\n",
              " 'hello what should eat for lunch',\n",
              " 'hello what do have to eat for supper',\n",
              " 'hi what do have to eat for brunch',\n",
              " 'hey what should eat for breakfast',\n",
              " 'hey what do have to eat for lunch',\n",
              " 'hi what is the diet plan for dinner',\n",
              " 'hey what the meal for supper',\n",
              " 'what should eat for brunch',\n",
              " 'what should eat for lunch',\n",
              " 'what should eat for breakfast',\n",
              " 'what do have to eat for lunch',\n",
              " 'what should eat for dinner',\n",
              " 'hello what do have to eat for brunch',\n",
              " 'what should eat for supper',\n",
              " 'hi what the diet for brunch',\n",
              " 'hey what should eat for lunch',\n",
              " 'hello what do have to eat for breakfast',\n",
              " 'hello what should eat for supper',\n",
              " 'what do have to eat for dinner',\n",
              " 'hey what is the diet plan for lunch',\n",
              " 'what is the diet for supper',\n",
              " 'hey what do have to eat for supper',\n",
              " 'hey what should eat for supper',\n",
              " 'what is the meal for breakfast',\n",
              " 'what the meal for supper',\n",
              " 'hi what the diet plan for breakfast',\n",
              " 'hi what the meal for dinner',\n",
              " 'hi what should eat for lunch',\n",
              " 'hi what the diet for lunch',\n",
              " 'hey what should eat for dinner',\n",
              " 'hello what the diet plan for dinner',\n",
              " 'what is the meal for lunch',\n",
              " 'what is the meal for brunch',\n",
              " 'hello what the meal for breakfast',\n",
              " 'hi what should eat for breakfast',\n",
              " 'what is the meal for supper',\n",
              " 'what is the diet plan for lunch',\n",
              " 'hey what is the meal for lunch',\n",
              " 'hello what the diet for lunch',\n",
              " 'hi what should eat for supper',\n",
              " 'hello what is the meal for breakfast',\n",
              " 'what the meal for dinner',\n",
              " 'hello what should eat for breakfast',\n",
              " 'what the diet plan for supper',\n",
              " 'hey what is the meal for dinner',\n",
              " 'what is the diet for dinner',\n",
              " 'what is the diet plan for dinner',\n",
              " 'hello what the meal for brunch',\n",
              " 'what the diet for dinner',\n",
              " 'what the diet for supper',\n",
              " 'what is the diet plan for breakfast',\n",
              " 'hi what is the diet for supper',\n",
              " 'hi what should eat for dinner',\n",
              " 'what the diet for lunch',\n",
              " 'hi what do have to eat for supper',\n",
              " 'hi what do have to eat for dinner',\n",
              " 'hello what is the meal for supper',\n",
              " 'what the meal for brunch',\n",
              " 'what is the meal for dinner',\n",
              " 'hey what do have to eat for breakfast',\n",
              " 'hi what should eat for brunch',\n",
              " 'what the meal for breakfast',\n",
              " 'hi what the diet plan for supper',\n",
              " 'hey what the diet for lunch',\n",
              " 'hello what is the meal for dinner',\n",
              " 'hi what is the diet plan for brunch',\n",
              " 'hey what is the meal for breakfast',\n",
              " 'hello what is the diet plan for dinner',\n",
              " 'hello what should eat for dinner',\n",
              " 'hi what is the meal for supper',\n",
              " 'hello what the meal for lunch',\n",
              " 'hi what the diet for dinner',\n",
              " 'what is the diet plan for supper',\n",
              " 'hello what is the meal for lunch',\n",
              " 'what the diet for breakfast',\n",
              " 'hi what do have to eat for breakfast',\n",
              " 'what the diet plan for lunch',\n",
              " 'hey what the meal for breakfast']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYuGTINn7630"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03SBpP3KVWuH",
        "outputId": "a6ee59d2-2e9f-4c5d-d0e9-3efd3c419764"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_df=0.7, max_features=1500, min_df=5,\n",
              "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
              "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
              "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
              "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
              "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
              "                            'itself', ...])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-HcXSYW8eb8",
        "outputId": "9459d7fc-6932-4df1-92b2-2260b2413edd"
      },
      "source": [
        "X[0],documents[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0]), 'how should work out for jul 1th')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxPmK0Az8fPD"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C__vzVs0WpdE",
        "outputId": "5f74d14a-79a4-432e-aaf0-4947bdd6a886"
      },
      "source": [
        "tfidfconverter\n",
        "joblib.dump(tfidfconverter, \"./tfidfconverter.joblib\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tfidfconverter.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2SwXpBI9DPD",
        "outputId": "0e7654cc-680e-4826-e51f-0e2da9f2f387"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38456403, 0.        , 0.        , ..., 0.        , 0.1587613 ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.44953494],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.32066209,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-NN10Jb9ELD"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zILyU5n9PUD",
        "outputId": "89ad1296-96d7-4f42-dd3a-392e5a780951"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "336    2\n",
              "64     0\n",
              "55     0\n",
              "106    0\n",
              "300    1\n",
              "      ..\n",
              "323    2\n",
              "192    1\n",
              "117    0\n",
              "47     0\n",
              "172    1\n",
              "Name: 1, Length: 320, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz0ryb-S9P8c",
        "outputId": "458baaa9-7795-4854-e0fc-9dc2293a589e"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
        "classifier.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=1000, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCsrxK2wIBhO",
        "outputId": "911667ea-f2bd-46da-c493-20f80ff9c8a4"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.50111608],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.37496577, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwThFmWX9Tzz"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0vBx7Kl9eEz",
        "outputId": "c26a08ef-c4f2-4126-b644-0f8f7cbec8a5"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[35  0  0]\n",
            " [ 0 31  1]\n",
            " [ 0  0 13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        35\n",
            "           1       1.00      0.97      0.98        32\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.99        80\n",
            "   macro avg       0.98      0.99      0.98        80\n",
            "weighted avg       0.99      0.99      0.99        80\n",
            "\n",
            "0.9875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHulIdH2OceR"
      },
      "source": [
        "import joblib\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L64lYBetOdfO",
        "outputId": "b76adbd0-d074-4321-f18e-158a15965c29"
      },
      "source": [
        "joblib.dump(classifier, \"./random_forest.joblib\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./random_forest.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOVkKor2Ojvo"
      },
      "source": [
        "loaded_rf = joblib.load(\"./random_forest.joblib\")\n",
        "y_pred = loaded_rf.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_1yfdawVNuq",
        "outputId": "1a5fca1b-bde3-4dd9-9337-43d303a4a1d3"
      },
      "source": [
        "joblib.dump(vectorizer, \"./vectorizer.joblib\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SBM_p6EVTFk"
      },
      "source": [
        "vectorizer = joblib.load(\"./vectorizer.joblib\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNIJPBJ2VooU",
        "outputId": "4baeab15-e54e-4073-f073-b585baceb1f4"
      },
      "source": [
        "vectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_df=0.7, max_features=1500, min_df=5,\n",
              "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
              "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
              "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
              "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
              "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
              "                            'itself', ...])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cUmYjayVcCg",
        "outputId": "56ddbdfd-297c-4ced-bab1-b66ec2b69b2c"
      },
      "source": [
        "type(vectorizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.feature_extraction.text.CountVectorizer"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "RUBAcCj798rB",
        "outputId": "cacc4809-9405-4d80-9074-165cde52f35a"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "# X = vectorizer.fit_transform(documents).toarray()\n",
        "docs_new = [\"What is my diet for lunch?\"]\n",
        "X_new_counts = vectorizer.transform(docs_new)\n",
        "X_new_tfidf = tfidfconverter.transform(X_new_counts)\n",
        "\n",
        "predicted = classifier.predict(X_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "  print('%r => %s' % (doc,our_dict[category]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3e4a6e8a23ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# X = vectorizer.fit_transform(documents).toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocs_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"What is my diet for lunch?\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_new_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_new_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             )\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixgIxSXjkmOP"
      },
      "source": [
        "## Second Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyIc4X6adXal"
      },
      "source": [
        "  import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import regex as re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDNjAo-Cj-Xj"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "data = pd.read_csv('data.csv',header=None,delimiter=\";\")\n",
        "# X, y = data[0],data[1]\n",
        "# X_train, y_train, X_test, y_test = train_test_split(X,y,test_size = 0.3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PL-63oYmDzF"
      },
      "source": [
        "#convert to lowercase, strip and remove punctuations\n",
        "def preprocess(text):\n",
        "    text = text.lower() \n",
        "    text=text.strip()  \n",
        "    text=re.compile('<.*?>').sub('', text) \n",
        "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
        "    text = re.sub('\\s+', ' ', text)  \n",
        "    # text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
        "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "    # text = re.sub(r'\\d',' ',text) \n",
        "    text = re.sub(r'\\s+',' ',text) \n",
        "    return text\n",
        "\n",
        " \n",
        "# STOPWORD REMOVAL\n",
        "def stopword(string):\n",
        "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
        "    return ' '.join(a)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        " \n",
        "# This is a helper function to map NTLK position tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Tokenize the sentence\n",
        "def lemmatizer(string):\n",
        "    word_pos_tags = nltk.pos_tag(word_tokenize(string))\n",
        "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] \n",
        "    return \" \".join(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH_lmUoMm2ow",
        "outputId": "c04b39de-cf0c-43fb-eca0-e493b1c70ca0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "htvZGX2umVGA",
        "outputId": "450cf2f7-416c-43e2-ff2d-5ba660fe181f"
      },
      "source": [
        "def finalpreprocess(string):\n",
        "  return lemmatizer(stopword(preprocess(string)))\n",
        "data['clean_text'] = data[0].apply(lambda x: finalpreprocess(x))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>how should I work out for jul 1th</td>\n",
              "      <td>0</td>\n",
              "      <td>jul 1th</td>\n",
              "      <td>work jul 1th</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is the workout on aug 3rd</td>\n",
              "      <td>0</td>\n",
              "      <td>aug 3rd</td>\n",
              "      <td>workout aug 3rd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what should I work out today</td>\n",
              "      <td>0</td>\n",
              "      <td>today</td>\n",
              "      <td>work today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hey what should I work out monday</td>\n",
              "      <td>0</td>\n",
              "      <td>monday</td>\n",
              "      <td>hey work monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hello how should I work out for june 2nd</td>\n",
              "      <td>0</td>\n",
              "      <td>june 2nd</td>\n",
              "      <td>hello work june 2nd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          0  1         2           clean_text\n",
              "0         how should I work out for jul 1th  0   jul 1th         work jul 1th\n",
              "1            what is the workout on aug 3rd  0   aug 3rd      workout aug 3rd\n",
              "2              what should I work out today  0     today           work today\n",
              "3         Hey what should I work out monday  0    monday      hey work monday\n",
              "4  hello how should I work out for june 2nd  0  june 2nd  hello work june 2nd"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz3bR5PTpsve"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[0],data[1],test_size=0.1,shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Cl6Taw7zEy",
        "outputId": "76a7d288-6c94-4730-a902-842a007e5867"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135                 what should I work out aug 3rd\n",
              "111             how should I work out for apr 07th\n",
              "251                  Hi what can I eat october 3rd\n",
              "206                 what should I eat the next day\n",
              "170              hi what do I have to eat tomorrow\n",
              "                          ...                     \n",
              "143       how should I work out for september 24th\n",
              "16              what is the workout plan on monday\n",
              "129            how should I work out for march 3rd\n",
              "137    Hey how should I work out for december 24th\n",
              "368               what is the diet plan for dinner\n",
              "Name: 0, Length: 360, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG2uwR4f772a"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol_vtLC574hE"
      },
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(X_train.map(lambda text, label: text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-0ldtY-f-s6"
      },
      "source": [
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(X_train).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFgyECxYvpfl",
        "outputId": "40c3e6b6-ffdd-408d-8e61-d2e7e21f0085"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 1, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3QQgTCAn3Xv"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"clean_text\"],data[1],test_size=0.2,shuffle=True)\n",
        "#Word2Vec\n",
        "# Word2Vec runs on tokenized sentences\n",
        "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
        "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VRoh-BcoK5O",
        "outputId": "7ee49769-16dc-4d65-de9a-cada337ba958"
      },
      "source": [
        "X_train_tok[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['workout', 'apr', '1th']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DKq17wLsQbk",
        "outputId": "fd1a8044-2a70-4ba7-bd57-42f886ddf738"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "216            eat tomorrow\n",
              "229              eat friday\n",
              "392        hello meal lunch\n",
              "135            work aug 3rd\n",
              "130    workout plan tuesday\n",
              "               ...         \n",
              "377           hi eat dinner\n",
              "124      hi work plan today\n",
              "20              work sunday\n",
              "113      hello work may 3rd\n",
              "180             eat tuesday\n",
              "Name: clean_text, Length: 320, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "lw5ctW5CoZ4d",
        "outputId": "1ef681f4-f818-49d6-8b74-213b0984651e"
      },
      "source": [
        "#Tf-Idf\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
        "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "#building Word2Vec model\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(next(iter(word2vec.values())))\n",
        "def fit(self, X, y):\n",
        "        return self\n",
        "def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "\n",
        "model = Word2Vec(data['clean_text_tok'],min_count=1)    \n",
        "w2v = dict(zip(model.wv.index2word, model.wv.syn0)) \n",
        "data['clean_text_tok']=[nltk.word_tokenize(i) for i in data['clean_text']] \n",
        "modelw = MeanEmbeddingVectorizer(w2v)\n",
        "# converting text to numerical data using Word2Vec\n",
        "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
        "X_val_vectors_w2v = modelw.transform(X_test_tok)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'clean_text_tok'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c5907d55fa0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         ])\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text_tok'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text_tok'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'clean_text_tok'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL8n8n9MmE9u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "011c5323-12b4-47f0-e867-65e5228de526"
      },
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-f332afe47faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# truncate and pad input sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_review_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    152\u001b[0m   return sequence.pad_sequences(\n\u001b[1;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m keras_export(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'eat tomorrow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egJiHaBMv9As",
        "outputId": "19629aa1-53d3-44ff-af19-3ff0390301dc"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 1, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHmpTyLX9cxX",
        "outputId": "9df76bd1-9764-4752-df2d-a52a3a0415a1"
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erHvGKaV_eX2"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam,SGD\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW9SMcBSj_tB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab72e3d7-72dd-47b3-ea3c-540cded1286d"
      },
      "source": [
        "lr = 8e-5\n",
        "decay_rate = 1e-4\n",
        "# create the model\n",
        "embedding_vecor_length = 16\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(1500, len(X), input_length=len(X[1])))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "opt = Adam(learning_rate=lr, decay=decay_rate)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X, y_train, epochs=300, batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, 45, 360)           540000    \n",
            "                                                                 \n",
            " spatial_dropout1d_9 (Spatia  (None, 45, 360)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 100)               184400    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 724,501\n",
            "Trainable params: 724,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "6/6 [==============================] - 4s 240ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 2/300\n",
            "6/6 [==============================] - 1s 238ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 3/300\n",
            "6/6 [==============================] - 1s 236ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 4/300\n",
            "6/6 [==============================] - 1s 236ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 5/300\n",
            "6/6 [==============================] - 1s 234ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 6/300\n",
            "6/6 [==============================] - 1s 236ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 7/300\n",
            "6/6 [==============================] - 1s 233ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 8/300\n",
            "6/6 [==============================] - 1s 236ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 9/300\n",
            "6/6 [==============================] - 1s 246ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 10/300\n",
            "6/6 [==============================] - 1s 230ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 11/300\n",
            "6/6 [==============================] - 1s 235ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 12/300\n",
            "6/6 [==============================] - 1s 241ms/step - loss: 0.0000e+00 - accuracy: 0.3917\n",
            "Epoch 13/300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-5e97cf59cf15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvyMihbVwe54"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9SUrqATkI0O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrwHYK2SkI2b"
      },
      "source": [
        "our_dict = {0:\"workout\",1:\"diet for next days\",2:\"diet for today\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sYfgwpskI2b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDymJr5dkI2c",
        "outputId": "6543f552-af28-44cd-8d25-e20ab26eb6b2"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahpY7yLtllxJ",
        "outputId": "c3724fd0-a8d3-4c7c-8167-125ccaab0041"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL6Px128lnah"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}